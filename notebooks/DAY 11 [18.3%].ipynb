{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 11 [18.3%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Differentially Private Query - Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_databases(db, num_databases = 5000):\n",
    "    \"\"\"\n",
    "    This function create a X number of databases from db,\n",
    "    where each new database has one missing row (element).\n",
    "    \n",
    "    params:\n",
    "    ------\n",
    "\n",
    "    db            -- Base database.\n",
    "    num_databases -- Number of database to create.\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    List of databases.\n",
    "    \"\"\"\n",
    "    databases = []\n",
    "    for k in range(0, num_databases):\n",
    "        databases.append(torch.cat([db[:k], db[k+1:]]))\n",
    "    return databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_and_parallels(num_entries):\n",
    "    \"\"\"\n",
    "    Create parallel databases.\n",
    "    \n",
    "    params:\n",
    "    ------\n",
    "    num_entries  -- Number of samples.\n",
    "    \n",
    "    returns:\n",
    "    -------\n",
    "    db           -- A single database.\n",
    "    databases    -- A database list.\n",
    "    \"\"\"\n",
    "    db = torch.rand(num_entries) > 0.5\n",
    "    databases = create_databases(db, num_entries)\n",
    "    return db, databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dbs(num_entries):\n",
    "    \"\"\"\n",
    "    Creates two databases using a random coin flip.\n",
    "    \n",
    "    params:\n",
    "    ------\n",
    "    num_entries    -- Total samples.\n",
    "    \n",
    "    returns:\n",
    "    -------\n",
    "    db             -- Normal data base,\n",
    "    db_            -- Database created by coin flips.\n",
    "    \"\"\"\n",
    "    db = torch.rand(num_entries) > 0.5\n",
    "    db_ = db.data.clone()\n",
    "    first_coin_flip = (torch.rand(len(db)) > .5).float()\n",
    "    second_coin_flip = (torch.rand(len(db)) > .5).float()\n",
    "    db_ = db.float() * first_coin_flip + (1 - first_coin_flip) * second_coin_flip\n",
    "    return db, db_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_query(db, threshold = 5):\n",
    "    # You need to cast this to float db.sum() > threshold\n",
    "    # otherwise it will recieve only binary values.\n",
    "    return (db.sum() > threshold).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(query, n_entries):\n",
    "    # Initialize a database and parallel databases\n",
    "    db, databases = create_db_and_parallels(n_entries)\n",
    "    # run query over the original db\n",
    "    #if flip() == 1: # tails\n",
    "    full_query_db = query(db)\n",
    "    # run query over the databases\n",
    "    sensitivity = 0\n",
    "    for index, pdb in enumerate(databases):\n",
    "        pbd_result = query(pdb)\n",
    "        db_distance = torch.abs(pbd_result - full_query_db) #L1 sensitity\n",
    "        if db_distance > sensitivity:\n",
    "            sensitivity = db_distance\n",
    "        print('For pdb{}, the sensitivity is {}'.format(index + 1, db_distance))\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_privacy(query, n_entries, epsilon):\n",
    "    # create the databse\n",
    "    # get the sensitivity\n",
    "    sensitivity, db = calculate_sensitivity(query, n_entries)\n",
    "    # calculate delta for laplacian\n",
    "    scaled_delta = sensitivity/epsilon\n",
    "    # set laplacian\n",
    "    laplacian_noise = np.random.laplace(0, scaled_delta)\n",
    "    # perform the query with noise\n",
    "    result = query(db)\n",
    "    noise_result = query(db) + laplacian_noise\n",
    "    print('Original query: {}'.format(result))\n",
    "    print('Query with laplacian noise: {}'.format(noise_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_query(db):\n",
    "    # You need to cast this to float db.sum() > threshold\n",
    "    # otherwise it will recieve only binary values.\n",
    "    return (db.sum()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_query(db):\n",
    "    return db.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_and_parallels(num_entries):\n",
    "    db = torch.rand(num_entries) > 0.5\n",
    "    databases = create_databases(db, num_entries)\n",
    "    return db, databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sensitivity(query, n_entries):\n",
    "    # Initialize a database and parallel databases\n",
    "    db, databases = create_db_and_parallels(n_entries)\n",
    "    # run query over the original db\n",
    "    #if flip() == 1: # tails\n",
    "        \n",
    "    full_query_db = query(db)\n",
    "    # run query over the databases\n",
    "    sensitivity = 0\n",
    "    for index, pdb in enumerate(databases):\n",
    "        pbd_result = query(pdb)\n",
    "        db_distance = torch.abs(pbd_result - full_query_db) #L1 sensitity\n",
    "        if db_distance > sensitivity:\n",
    "            sensitivity = db_distance\n",
    "        #print('For pdb{}, the sensitivity is {}'.format(index + 1, db_distance))\n",
    "    return sensitivity, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pdb1, the sensitivity is 1.0\n",
      "For pdb2, the sensitivity is 1.0\n",
      "For pdb3, the sensitivity is 1.0\n",
      "For pdb4, the sensitivity is 1.0\n",
      "For pdb5, the sensitivity is 1.0\n",
      "For pdb6, the sensitivity is 0.0\n",
      "For pdb7, the sensitivity is 1.0\n",
      "For pdb8, the sensitivity is 1.0\n",
      "For pdb9, the sensitivity is 0.0\n",
      "For pdb10, the sensitivity is 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity(sum_query, n_entries = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5 # how much information to leak. Here it says leaks about 50% of information.\n",
    "# If we set e to 0.2, it would mean, leak 20% of information.\n",
    "# Lower the epsilon, greater the noise.\n",
    "# Also, epsilon is refered as the privacy budget, it could be a greater number like 5.\n",
    "# In that case one can assing a portion of epsilon for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: 46.0\n",
      "Query with laplacian noise: 44.376075744628906\n"
     ]
    }
   ],
   "source": [
    "global_privacy(sum_query, n_entries = 100, epsilon = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: 48.0\n",
      "Query with laplacian noise: 135320.265625\n"
     ]
    }
   ],
   "source": [
    "global_privacy(sum_query, n_entries = 100, epsilon = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: 0.4699999988079071\n",
      "Query with laplacian noise: 0.4605430066585541\n"
     ]
    }
   ],
   "source": [
    "global_privacy(mean_query, n_entries = 100, epsilon = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: 0.4699999988079071\n",
      "Query with laplacian noise: 10.188638687133789\n"
     ]
    }
   ],
   "source": [
    "global_privacy(mean_query, n_entries = 100, epsilon = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
